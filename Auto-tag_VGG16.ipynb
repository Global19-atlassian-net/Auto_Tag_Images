{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If you think this notebook is worth reading and has gained some knowledge from this,please consider upvoting my kernel.Your appreciation means a lot to me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed value\n",
    "# Apparently you may use different seed values at each stage\n",
    "seed_value= 48\n",
    "\n",
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "import tensorflow as tf\n",
    "# tf.random.set_seed(seed_value)\n",
    "# for later versions: \n",
    "tf.compat.v1.set_random_seed(seed_value)\n",
    "\n",
    "# 5. Configure a new global `tensorflow` session\n",
    "# from keras import backend as K\n",
    "# session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "# sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "# K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train and test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>image7042.jpg</td>\n",
       "      <td>Food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>image3327.jpg</td>\n",
       "      <td>misc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>image10335.jpg</td>\n",
       "      <td>Attire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>image8019.jpg</td>\n",
       "      <td>Food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>image2128.jpg</td>\n",
       "      <td>Attire</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Image   Class\n",
       "0   image7042.jpg    Food\n",
       "1   image3327.jpg    misc\n",
       "2  image10335.jpg  Attire\n",
       "3   image8019.jpg    Food\n",
       "4   image2128.jpg  Attire"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=pd.read_csv('/kaggle/input/classification-of-images/dataset/train.csv')\n",
    "test=pd.read_csv('/kaggle/input/classification-of-images/dataset/test.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert categorical to numerical for training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Class_map={'Food':0,'Attire':1,'Decorationandsignage':2,'misc':3}\n",
    "inverse_map={0:'Food',1:'Attire',2:'Decorationandsignage',3:'misc'}\n",
    "train['Class']=train['Class'].map(Class_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       3\n",
       "2       1\n",
       "3       0\n",
       "4       1\n",
       "       ..\n",
       "5978    0\n",
       "5979    1\n",
       "5980    0\n",
       "5981    0\n",
       "5982    1\n",
       "Name: Class, Length: 5983, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5983/5983 [00:13<00:00, 444.77it/s]\n"
     ]
    }
   ],
   "source": [
    "train_img=[]\n",
    "train_label=[]\n",
    "j=0\n",
    "path='/kaggle/input/classification-of-images/dataset/Train Images'\n",
    "for i in tqdm(train['Image']):\n",
    "    final_path=os.path.join(path,i)\n",
    "    img=cv2.imread(final_path)\n",
    "    img=cv2.resize(img,(128,128))\n",
    "    img=img.astype('float32')\n",
    "    img = img - [103.939, 116.779, 123.68]\n",
    "    train_img.append(img)\n",
    "    train_label.append(train['Class'][j])\n",
    "    j=j+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image agumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.1, # Randomly zoom image \n",
    "        width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "new_datagen = ImageDataGenerator()\n",
    "\n",
    "datagen.fit(train_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3219/3219 [00:07<00:00, 439.37it/s]\n"
     ]
    }
   ],
   "source": [
    "test_img=[]\n",
    "path='/kaggle/input/classification-of-images/dataset/Test Images'\n",
    "for i in tqdm(test['Image']):\n",
    "    final_path=os.path.join(path,i)\n",
    "    img=cv2.imread(final_path)\n",
    "    img=cv2.resize(img,(128,128))\n",
    "    img=img.astype('float32')\n",
    "    img = img - [103.939, 116.779, 123.68]\n",
    "    test_img.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5728, 128, 128, 3)\n",
      "(3219, 128, 128, 3)\n",
      "(5728,)\n"
     ]
    }
   ],
   "source": [
    "train_img=np.array(train_img)\n",
    "test_img=np.array(test_img)\n",
    "train_label=np.array(train_label)\n",
    "print(train_img.shape)\n",
    "print(test_img.shape)\n",
    "print(train_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight('balanced', np.unique(train_label), train_label)\n",
    "d_class_weights = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "\n",
    "from tensorflow.keras.layers import Flatten,Dense,Dropout,BatchNormalization\n",
    "from tensorflow.keras.models import Model,Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vgg16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model=VGG16(include_top=False, weights='imagenet',input_shape=(128,128,3), pooling='avg')\n",
    "model=Sequential()\n",
    "model.add(base_model)\n",
    "model.add(Dense(1024,activation='relu'))\n",
    "model.add(Dense(4,activation='softmax'))\n",
    "\n",
    "\n",
    "from keras.optimizers import Adam,SGD,Adagrad,Adadelta,RMSprop\n",
    "\n",
    "base_model.trainable = False\n",
    "\n",
    "reduce_learning_rate = ReduceLROnPlateau(monitor='loss',\n",
    "                                         factor=0.1,\n",
    "                                         patience=2,\n",
    "                                         cooldown=2,\n",
    "                                         min_lr=0.00001,\n",
    "                                         verbose=1)\n",
    "\n",
    "callbacks = [reduce_learning_rate]\n",
    "    \n",
    "\n",
    "\n",
    "model.compile( optimizer='adam'),loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 358 steps\n",
      "Epoch 1/30\n",
      "358/358 [==============================] - 26s 74ms/step - loss: 1.6408 - accuracy: 0.6163\n",
      "Epoch 2/30\n",
      "358/358 [==============================] - 22s 63ms/step - loss: 0.8799 - accuracy: 0.6789\n",
      "Epoch 3/30\n",
      "358/358 [==============================] - 22s 61ms/step - loss: 0.8799 - accuracy: 0.6805\n",
      "Epoch 4/30\n",
      "358/358 [==============================] - 23s 63ms/step - loss: 0.8331 - accuracy: 0.7057\n",
      "Epoch 5/30\n",
      "358/358 [==============================] - 22s 62ms/step - loss: 0.8097 - accuracy: 0.7200\n",
      "Epoch 6/30\n",
      "358/358 [==============================] - 22s 61ms/step - loss: 0.7492 - accuracy: 0.7394\n",
      "Epoch 7/30\n",
      "358/358 [==============================] - 23s 63ms/step - loss: 0.7589 - accuracy: 0.7453\n",
      "Epoch 8/30\n",
      "357/358 [============================>.] - ETA: 0s - loss: 0.7510 - accuracy: 0.7507\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "358/358 [==============================] - 22s 62ms/step - loss: 0.7529 - accuracy: 0.7498\n",
      "Epoch 9/30\n",
      "358/358 [==============================] - 22s 61ms/step - loss: 0.5669 - accuracy: 0.7964\n",
      "Epoch 10/30\n",
      "358/358 [==============================] - 23s 64ms/step - loss: 0.4848 - accuracy: 0.8244\n",
      "Epoch 11/30\n",
      "358/358 [==============================] - 22s 61ms/step - loss: 0.4292 - accuracy: 0.8396\n",
      "Epoch 12/30\n",
      "358/358 [==============================] - 22s 62ms/step - loss: 0.4192 - accuracy: 0.8425\n",
      "Epoch 13/30\n",
      "358/358 [==============================] - 22s 63ms/step - loss: 0.3858 - accuracy: 0.8534\n",
      "Epoch 14/30\n",
      "358/358 [==============================] - 22s 61ms/step - loss: 0.3663 - accuracy: 0.8616\n",
      "Epoch 15/30\n",
      "358/358 [==============================] - 23s 63ms/step - loss: 0.3467 - accuracy: 0.8656\n",
      "Epoch 16/30\n",
      "358/358 [==============================] - 22s 62ms/step - loss: 0.3304 - accuracy: 0.8736\n",
      "Epoch 17/30\n",
      "358/358 [==============================] - 22s 61ms/step - loss: 0.3157 - accuracy: 0.8783\n",
      "Epoch 18/30\n",
      "358/358 [==============================] - 23s 64ms/step - loss: 0.3061 - accuracy: 0.8774\n",
      "Epoch 19/30\n",
      "358/358 [==============================] - 22s 62ms/step - loss: 0.2978 - accuracy: 0.8867\n",
      "Epoch 20/30\n",
      "358/358 [==============================] - 22s 62ms/step - loss: 0.2844 - accuracy: 0.8921\n",
      "Epoch 21/30\n",
      "358/358 [==============================] - 23s 64ms/step - loss: 0.2821 - accuracy: 0.8921\n",
      "Epoch 22/30\n",
      "358/358 [==============================] - 22s 61ms/step - loss: 0.2682 - accuracy: 0.8918\n",
      "Epoch 23/30\n",
      "358/358 [==============================] - 23s 64ms/step - loss: 0.2589 - accuracy: 0.9005\n",
      "Epoch 24/30\n",
      "358/358 [==============================] - 22s 62ms/step - loss: 0.2401 - accuracy: 0.9052\n",
      "Epoch 25/30\n",
      "358/358 [==============================] - 22s 61ms/step - loss: 0.2302 - accuracy: 0.9080\n",
      "Epoch 26/30\n",
      "358/358 [==============================] - 23s 64ms/step - loss: 0.2138 - accuracy: 0.9152\n",
      "Epoch 27/30\n",
      "358/358 [==============================] - 22s 61ms/step - loss: 0.2248 - accuracy: 0.9120\n",
      "Epoch 28/30\n",
      "358/358 [==============================] - 22s 61ms/step - loss: 0.2040 - accuracy: 0.9216\n",
      "Epoch 29/30\n",
      "358/358 [==============================] - 23s 63ms/step - loss: 0.2159 - accuracy: 0.9134\n",
      "Epoch 30/30\n",
      "357/358 [============================>.] - ETA: 0s - loss: 0.2053 - accuracy: 0.9186\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "358/358 [==============================] - 22s 62ms/step - loss: 0.2051 - accuracy: 0.9186\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f2e58cad6a0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(datagen.flow(train_img, to_categorical(train_label,4), batch_size=16),\n",
    "                    epochs=30,callbacks=callbacks, class_weight=d_class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(datagen.flow(train_img, to_categorical(train_label,4), batch_size=16),\n",
    "                    epochs=10,callbacks=callbacks, class_weight=d_class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict for test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.6501023e-01 2.0476555e-02 1.1134567e-02 3.3786672e-03]\n",
      " [5.1619369e-01 6.1776731e-03 3.3043497e-04 4.7729814e-01]\n",
      " [9.3413365e-01 6.4882013e-05 2.3955388e-07 6.5801166e-02]\n",
      " [9.9999237e-01 1.3487572e-10 3.8153205e-13 7.6715914e-06]]\n",
      "['Food', 'Food', 'Food']\n"
     ]
    }
   ],
   "source": [
    "labels = model.predict(test_img)\n",
    "print(labels[:4])\n",
    "label = [np.argmax(i) for i in labels]\n",
    "class_label = [inverse_map[x] for x in label]\n",
    "print(class_label[:3])\n",
    "submission = pd.DataFrame({ 'Image': test.Image, 'Class': class_label })\n",
    "submission.head(10)\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
